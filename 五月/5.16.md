# 机器学习

### ResNet

- [详解深度学习之经典网络架构（六）：ResNet 两代（ResNet v1和ResNet v2）_chenyuping666的博客-CSDN博客_resnet v2](https://blog.csdn.net/chenyuping333/article/details/82344334?ops_request_misc=%7B%22request%5Fid%22%3A%22165267173916782395377894%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165267173916782395377894&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-82344334-null-null.142^v9^pc_search_result_cache,157^v4^control&utm_term=resnetv2&spm=1018.2226.3001.4187)

### CNN

#### 网站

- **[CS231n 用于视觉识别的卷积神经网络](https://cs231n.github.io/convolutional-networks/)**
- [shanglianlm0525/PyTorch-Networks: Pytorch implementation of cnn network (github.com)](https://github.com/shanglianlm0525/PyTorch-Networks) **CNN网络的pytorch实现**

#### 参数共享

##### 疑问

- 共享的是什么参数？
- 为什么共享参数不会影响到特征提取？

##### 资料

- [CNN中的卷积操作与参数共享_☞源仔的博客-CSDN博客_卷积操作实现的参数共享是指多个卷积层上的参数共享](https://blog.csdn.net/weixin_54546190/article/details/122179752?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1-122179752-blog-53241133.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1-122179752-blog-53241133.pc_relevant_default&utm_relevant_index=2)
- [一张图理解卷积层的参数共享_格林深瞳的博客-CSDN博客_参数共享](https://blog.csdn.net/ture_dream/article/details/53241133?ops_request_misc=%7B%22request%5Fid%22%3A%22165267535816781685349565%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165267535816781685349565&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-53241133-null-null.142^v9^pc_search_result_cache,157^v4^control&utm_term=卷积神经网络为什么要参数共享&spm=1018.2226.3001.4187)
- [卷积神经网络(CNN)笔记（五）—— 参数共享机制_zeeq_的博客-CSDN博客_参数共享机制](https://blog.csdn.net/weixin_44120025/article/details/114850627?ops_request_misc=%7B%22request%5Fid%22%3A%22165267535816781685349565%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165267535816781685349565&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-114850627-null-null.142^v9^pc_search_result_cache,157^v4^control&utm_term=卷积神经网络为什么要参数共享&spm=1018.2226.3001.4187)
  - 如果一个特征在某个空间位置(x,y)上的计算是有用的，那么在另一个位置(x2,y2)上的计算也是有用的。

#### 卷积核随机初始化

##### 疑问

- 为什么一个卷积核对应一个特征图？
- 为什么同一张图不同的卷积核学到的特征图不同？

##### 资料

- [卷积神经网络中卷积核是如何学习到特征的？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/430129801)

#### 卷积操作

##### 疑问

- 多通道卷积时，用不同的卷积核对每一个通道进行卷积，用激活函数合并特征是什么意思？
  - [多通道(比如RGB三通道)卷积过程_deep_learninger的博客-CSDN博客_多通道卷积](https://blog.csdn.net/u014114990/article/details/51125776?ops_request_misc=%7B%22request%5Fid%22%3A%22165267790916782390517023%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165267790916782390517023&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-51125776-null-null.142^v9^pc_search_result_cache,157^v4^control&utm_term=多通道卷积&spm=1018.2226.3001.4187)
  - 依然是比如十个核，一个核对三个通道分别卷积后合并（应该）
- 神经网络图中卷积核体现在哪？
  - 就是说卷积层那几个神经代表的是什么

### Inception

#### 知识

- [Google Inception Net介绍_修炼之路的博客-CSDN博客_google inception net](https://blog.csdn.net/sinat_29957455/article/details/80766850?ops_request_misc=%7B%22request%5Fid%22%3A%22165268020116780357226291%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165268020116780357226291&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-80766850-null-null.142^v9^pc_search_result_cache,157^v4^control&utm_term=inceptionnet&spm=1018.2226.3001.4187)

- [[深度学习\]Inception Net （V1-V4）系列论文笔记_Thorrrrrrrrrr的博客-CSDN博客_inceptionnet](https://blog.csdn.net/sinat_33487968/article/details/83588372?ops_request_misc=%7B%22request%5Fid%22%3A%22165268090216780366537156%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165268090216780366537156&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-83588372-null-null.142^v9^pc_search_result_cache,157^v4^control&utm_term=inceptionnet-v1&spm=1018.2226.3001.4187)
- [卷积神经网络学习路线（三）| 盘点不同类型的池化层、1*1卷积的作用和卷积核是否一定越大越好？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/96528331) 复现的时候刚好遇到重叠池化，我看了一眼
- [“logits”到底是个什么意思？_AlexXuZe的博客-CSDN博客_logits](https://blog.csdn.net/nbxzkok/article/details/84838984?ops_request_misc=%7B%22request%5Fid%22%3A%22165269845916781818797925%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165269845916781818797925&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-84838984-null-null.142^v9^pc_search_result_cache,157^v4^control&utm_term=logits&spm=1018.2226.3001.4187) 复现的代码里有一个变量名用了logit提示字典没有…好奇就看了一下

#### 代码

- [PyTorch实现的GoogLeNet (InceptionV1)_mingo_敏的博客-CSDN博客_inceptionv1 pytorch](https://blog.csdn.net/shanglianlm/article/details/99005326?ops_request_misc=%7B%22request%5Fid%22%3A%22165268220216782425182091%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165268220216782425182091&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-3-99005326-null-null.142^v9^pc_search_result_cache,157^v4^control&utm_term=inceptionv1++pytorch&spm=1018.2226.3001.4187)
  - 我觉得这个下面的代码块图非常不错
  - 尤其是这代码来自一个GitHub项目，里面有好多CNN的pytorch实现，非常棒
  - 但是我不是很清楚为什么要有中间模块InceptionAux这个辅助分类器，有空再想想
  - 另外就是还是觉得自己对这些功能层到底是怎么设计分配的不够清楚，比如每一层的参数，层与层的先后关系等，所以打算多复现几个网络来熟悉熟悉
  - 比如有的卷积层用了padding，有的木有……
-   [test.py](test.py) 
  - 这就是我复现的，一模一样……不过稍微能理解一下结构，但是很不幸，其中InceptionV1Module模块的参数为什么是这样完全还不理解，希望能了解一下
  - 同时因为这些奇奇怪怪的参数我抄错了好几个地方……检查了半天

### 书籍

- [《Python深度学习实战:基于Pytorch》 - 当当 -  (dangdang.com)](http://product.dangdang.com/11134506556.html) 
  - 最近在看这本，感觉讲好细，之前学过几次的东西还能被带着找到重要的细节

# 玩

- 最近在玩一个放置游戏……无敌套娃，剧情还是挺有意思的，刚好快通关了（**99.6%**），分享一下，适合学习时点几下~~分散注意力~~
- **《Increlution》**，是steam上的，有汉化教程 [Increlution - Git游戏 - gityx.com (g8hh.com)](https://www.g8hh.com/#/game/increlution)

