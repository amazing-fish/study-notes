# 机器学习

### NLP

#### NNLM

- https://jmlr.org/papers/v3/bengio03a.html
  - 书上说是NLP入门必读论文……还没仔细看

- [白叶琪/word_language_model (github.com)](https://github.com/BAI-Yeqi/word_language_model) 
  - 找到的一个pytorch实现……也还没仔细看
- [A Neural Probabilistic Language Model(文献阅读笔记) -CSDN博客](https://blog.csdn.net/NINJA_xu/article/details/117660476?ops_request_misc=%7B%22request%5Fid%22%3A%22165283963216782425116820%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165283963216782425116820&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-117660476-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=A+Neural+Probabilistic+Language+Model&spm=1018.2226.3001.4187) 

#### Word Embedding

- [Word Embedding（一）NNLM、word2vec、GloVe_耩豇的博客-CSDN博客](https://blog.csdn.net/qq_33858719/article/details/93356042?ops_request_misc=%7B%22request%5Fid%22%3A%22165284980916781685319052%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165284980916781685319052&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-93356042-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=nnlm和embedding&spm=1018.2226.3001.4187)
  - 没细看

- [如何通俗理解word2vec_v_JULY_v的博客-CSDN博客_word2vec](https://blog.csdn.net/v_JULY_v/article/details/102708459?ops_request_misc=%7B%22request%5Fid%22%3A%22165284927116781818723065%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165284927116781818723065&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-102708459-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=Word2Vec&spm=1018.2226.3001.4187) 
  - 讲的很详细，数学分析还没看（晕）
- [理解GloVe模型（+总结）_AI蜗牛之家的博客-CSDN博客_glove模型](https://blog.csdn.net/u014665013/article/details/79642083?ops_request_misc=%7B%22request%5Fid%22%3A%22165284928216781432998188%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165284928216781432998188&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-79642083-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=GloVe&spm=1018.2226.3001.4187) 
  - 还没细看


#### RNN

##### LSTM

- [LSTM原理及实现（一）_bill_b的博客-CSDN博客](https://blog.csdn.net/weixin_44162104/article/details/88660003?ops_request_misc=%7B%22request%5Fid%22%3A%22165283086116781432951546%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165283086116781432951546&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-88660003-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=LSTM&spm=1018.2226.3001.4187)
  - 感觉为了解决梯度传播的方法都和残差是同一个思想

- [了解LSTM网络 -- colah的博客](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  - 然后发现了这一篇，讲的比较全……colah这网站好像挺强

##### GUR

- 看到说和LSTM没有实践上的大区别，就不打算细看了

#### RNNLM

- [Recurrent Neural Network Based Language Model (vutbr.cz)](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
  - 先把论文收着，到时候再看
- [语言模型（三）—— 循环神经网络语言模型（RNNLM）与语言模型评价指标_-CSDN博客](https://blog.csdn.net/rongsenmeng2835/article/details/108656674?ops_request_misc=%7B%22request%5Fid%22%3A%22165284486716781435438842%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165284486716781435438842&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-108656674-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=rnnlm&spm=1018.2226.3001.4187) 

#### Seq2Seq

- [动手学深度学习（四十四）——Seq2Seq原理与实现_留小星的博客-CSDN博客_seq2seq损失](https://blog.csdn.net/jerry_liufeng/article/details/121342928?ops_request_misc=%7B%22request%5Fid%22%3A%22165285107916782388097573%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165285107916782388097573&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-121342928-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=seq2seq&spm=1018.2226.3001.4187) 
  - 没看实现
- [深度学习中的注意力机制_csdn大数据的博客-CSDN博客](https://blog.csdn.net/TG229dvt5I93mxaQ5A6U/article/details/78422216?ops_request_misc=%7B%22request%5Fid%22%3A%22165283395416781818771530%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165283395416781818771530&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-78422216-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=注意力机制&spm=1018.2226.3001.4187) 

#### ConvSeq2Seq

- [Convolutional Sequence to Sequence Learning (mlr.press)](http://proceedings.mlr.press/v70/gehring17a.html)
  - flag: 有空看

# 笔记

### 软件

#### typora

- 今天闲着没事觉得跳出来的激活界面太烦了，就直接买了，谁是大冤种我不说🥺
- 然后就去查了使用说明，不能白买
- **[Typora功能汇总 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/483671352)**

# 今日总结

- 今天开始看NLP了，虽然才刚开始，但已经遇到了好多重要的论文和发展转折点
- 感觉得慢慢来，周末可以抽空看一看几篇论文，以及代码的实现（虽然知道这方面的训练好像要很久）

