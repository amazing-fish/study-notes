# 机器学习

### NLP

#### NNLM

- https://jmlr.org/papers/v3/bengio03a.html
  - 书上说是NLP入门必读论文……还没仔细看

- [白叶琪/word_language_model (github.com)](https://github.com/BAI-Yeqi/word_language_model) 
  - 找到的一个pytorch实现……也还没仔细看
- [A Neural Probabilistic Language Model(文献阅读笔记) -CSDN博客](https://blog.csdn.net/NINJA_xu/article/details/117660476?ops_request_misc=%7B%22request%5Fid%22%3A%22165283963216782425116820%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165283963216782425116820&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-117660476-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=A+Neural+Probabilistic+Language+Model&spm=1018.2226.3001.4187) 

#### Word Embedding

- [Word Embedding（一）NNLM、word2vec、GloVe_耩豇的博客-CSDN博客](https://blog.csdn.net/qq_33858719/article/details/93356042?ops_request_misc=%7B%22request%5Fid%22%3A%22165284980916781685319052%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165284980916781685319052&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-93356042-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=nnlm和embedding&spm=1018.2226.3001.4187)
  - 没细看

- [如何通俗理解word2vec_v_JULY_v的博客-CSDN博客_word2vec](https://blog.csdn.net/v_JULY_v/article/details/102708459?ops_request_misc=%7B%22request%5Fid%22%3A%22165284927116781818723065%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165284927116781818723065&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-102708459-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=Word2Vec&spm=1018.2226.3001.4187) 
  - 讲的很详细，数学分析还没看（晕）
- [理解GloVe模型（+总结）_AI蜗牛之家的博客-CSDN博客_glove模型](https://blog.csdn.net/u014665013/article/details/79642083?ops_request_misc=%7B%22request%5Fid%22%3A%22165284928216781432998188%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165284928216781432998188&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-79642083-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=GloVe&spm=1018.2226.3001.4187) 
  - 还没细看


#### RNN

##### LSTM

- [LSTM原理及实现（一）_bill_b的博客-CSDN博客](https://blog.csdn.net/weixin_44162104/article/details/88660003?ops_request_misc=%7B%22request%5Fid%22%3A%22165283086116781432951546%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165283086116781432951546&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-88660003-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=LSTM&spm=1018.2226.3001.4187)
  - 感觉为了解决梯度传播的方法都和残差是同一个思想

- [了解LSTM网络 -- colah的博客](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  - 然后发现了这一篇，讲的比较全……colah这网站好像挺强

##### GUR

- 看到说和LSTM没有实践上的大区别，就不打算细看了

#### RNNLM

- [Recurrent Neural Network Based Language Model (vutbr.cz)](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
  - 先把论文收着，到时候再看
- [语言模型（三）—— 循环神经网络语言模型（RNNLM）与语言模型评价指标_-CSDN博客](https://blog.csdn.net/rongsenmeng2835/article/details/108656674?ops_request_misc=%7B%22request%5Fid%22%3A%22165284486716781435438842%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165284486716781435438842&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-108656674-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=rnnlm&spm=1018.2226.3001.4187) 

#### Seq2Seq

- [动手学深度学习（四十四）——Seq2Seq原理与实现_留小星的博客-CSDN博客_seq2seq损失](https://blog.csdn.net/jerry_liufeng/article/details/121342928?ops_request_misc=%7B%22request%5Fid%22%3A%22165285107916782388097573%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165285107916782388097573&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-121342928-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=seq2seq&spm=1018.2226.3001.4187) 
  - 没看实现
- [深度学习中的注意力机制_csdn大数据的博客-CSDN博客](https://blog.csdn.net/TG229dvt5I93mxaQ5A6U/article/details/78422216?ops_request_misc=%7B%22request%5Fid%22%3A%22165283395416781818771530%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165283395416781818771530&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-78422216-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=注意力机制&spm=1018.2226.3001.4187) 

#### ConvSeq2Seq

- [Convolutional Sequence to Sequence Learning (mlr.press)](http://proceedings.mlr.press/v70/gehring17a.html)
  - flag: 有空看
- [基于CNN的Seq2Seq模型-Convolutional Sequence to Sequence_修炼打怪的小乌龟的博客-CSDN博客_convseq2seq](https://blog.csdn.net/u010417185/article/details/83089986?ops_request_misc=%7B%22request%5Fid%22%3A%22165285742416782350975852%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165285742416782350975852&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-6-83089986-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=ConvSeq2Seq&spm=1018.2226.3001.4187)
- ~~[facebookresearch/fairseq：用Python编写的Facebook AI Research Sequence-to-Sequence Toolkit。 (github.com)](https://github.com/facebookresearch/fairseq)~~
  - ~~看到一个Facebook的开源库~~
  - 刚看了一下好像不支持Windows？
- **[机器翻译模型五CNN+seq2seq__Pytorch实现_散人stu174的博客-CSDN博客_cnn seq2seq](https://blog.csdn.net/weixin_38544305/article/details/115788602?ops_request_misc=%7B%22request%5Fid%22%3A%22165286506116782246495847%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165286506116782246495847&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-115788602-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=conv_seq2seq+pytorch实现&spm=1018.2226.3001.4187)**
  - 重新找了个代码带讲解，感觉挺不错
  - 下面是看讲解时又遇到的问题
  - 为什么经过glu激活后维度会降一半？
  - **焯，因为版本问题，有的方法无法使用，目前只能降版本……但是不知道作者是哪个版本**，暂时放一下


- [GLU（Gated Linear Unit，门控线性单元）简介_coder1479的博客-CSDN博客_门控线性单元](https://blog.csdn.net/m0_48742971/article/details/123431686?ops_request_misc=%7B%22request%5Fid%22%3A%22165286664716781818744507%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165286664716781818744507&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-123431686-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=GLU&spm=1018.2226.3001.4187) 
- [Language Modeling with Gated Convolutional Networks(句子建模之门控CNN）--模型简介篇_liuchongee的博客-CSDN博客](https://blog.csdn.net/liuchonge/article/details/70238350) 


# 资源

### 软件

#### typora

- 今天闲着没事觉得跳出来的激活界面太烦了，就直接买了，谁是大冤种我不说🥺
- 然后就去查了使用说明，不能白买
- **[Typora功能汇总 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/483671352)** 

### python库

- spaCy 
  - [NLP in Python(spaCy模块的简单使用)_Janvn的博客-CSDN博客_python spacy](https://blog.csdn.net/u013709332/article/details/99700332?ops_request_misc=%7B%22request%5Fid%22%3A%22165286883516781685314217%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165286883516781685314217&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-99700332-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=spaCy&spm=1018.2226.3001.4187) 
  - 好像是类似NLP里的numpy
- torchtext
  - [【Pytorch】【torchtext(一)】概述与基本操作_BQW_的博客-CSDN博客](https://blog.csdn.net/bqw18744018044/article/details/109149646?ops_request_misc=&request_id=&biz_id=102&utm_term=torchtext&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-109149646.nonecase&spm=1018.2226.3001.4187) 
  - 好像是pytorch的语料库，用到的时候再细看
  - **焯，有大问题**

# 今日总结

- 今天开始看NLP了，虽然才刚开始，但已经遇到了好多重要的论文和发展转折点
- 感觉得慢慢来，周末可以抽空看一看几篇论文，以及代码的实现（虽然知道这方面的训练好像要很久）
- 本来想复现一个代码结果因为库的更新反复横跳的函数，没法搞。得有时间去研究了

