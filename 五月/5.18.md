# 机器学习

### NLP

#### NNLM

##### 结构

- https://jmlr.org/papers/v3/bengio03a.html
  - 书上说是NLP入门必读论文……还没仔细看

- [白叶琪/word_language_model (github.com)](https://github.com/BAI-Yeqi/word_language_model) 
  - 找到的一个pytorch实现……也还没仔细看
- [A Neural Probabilistic Language Model(文献阅读笔记) -CSDN博客](https://blog.csdn.net/NINJA_xu/article/details/117660476?ops_request_misc=%7B%22request%5Fid%22%3A%22165283963216782425116820%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165283963216782425116820&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-117660476-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=A+Neural+Probabilistic+Language+Model&spm=1018.2226.3001.4187) 

##### Word Embedding

- [Word Embedding（一）NNLM、word2vec、GloVe_耩豇的博客-CSDN博客](https://blog.csdn.net/qq_33858719/article/details/93356042?ops_request_misc=%7B%22request%5Fid%22%3A%22165284980916781685319052%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165284980916781685319052&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-93356042-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=nnlm和embedding&spm=1018.2226.3001.4187)
  - 没细看

- [如何通俗理解word2vec_v_JULY_v的博客-CSDN博客_word2vec](https://blog.csdn.net/v_JULY_v/article/details/102708459?ops_request_misc=%7B%22request%5Fid%22%3A%22165284927116781818723065%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165284927116781818723065&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-102708459-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=Word2Vec&spm=1018.2226.3001.4187) 
  - 讲的很详细，数学分析还没看（晕）
- [理解GloVe模型（+总结）_AI蜗牛之家的博客-CSDN博客_glove模型](https://blog.csdn.net/u014665013/article/details/79642083?ops_request_misc=%7B%22request%5Fid%22%3A%22165284928216781432998188%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165284928216781432998188&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-79642083-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=GloVe&spm=1018.2226.3001.4187) 
  - 还没细看


#### RNN

##### LSTM

- [LSTM原理及实现（一）_bill_b的博客-CSDN博客](https://blog.csdn.net/weixin_44162104/article/details/88660003?ops_request_misc=%7B%22request%5Fid%22%3A%22165283086116781432951546%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165283086116781432951546&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-88660003-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=LSTM&spm=1018.2226.3001.4187)
  - 感觉为了解决梯度传播的方法都和残差是同一个思想

- [了解LSTM网络 -- colah的博客](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  - 然后发现了这一篇，讲的比较全……colah这网站好像挺强

##### GUR

- 看到说和LSTM没有实践上的大区别，就不打算细看了

#### RNNLM

- [Recurrent Neural Network Based Language Model (vutbr.cz)](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
  - 先把论文收着，到时候再看
- [语言模型（三）—— 循环神经网络语言模型（RNNLM）与语言模型评价指标_-CSDN博客](https://blog.csdn.net/rongsenmeng2835/article/details/108656674?ops_request_misc=%7B%22request%5Fid%22%3A%22165284486716781435438842%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165284486716781435438842&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-108656674-null-null.142^v10^pc_search_result_control_group,157^v4^control&utm_term=rnnlm&spm=1018.2226.3001.4187) 

# 笔记

### 软件

#### typora

- 今天闲着没事觉得跳出来的激活界面太烦了，就直接买了，谁是大冤种我不说🥺
- 然后就去查了使用说明，不能白买
- **[Typora功能汇总 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/483671352)**

