# 模式识别

## EM算法

### 推导

- 首先就是为什么要用EM：
  - 如在GMM中，有多个高斯模型，参数很多，用最大似然估计时，log里有连加计算，求导很难
  - 所以要使用递归方法就是EM
- 递归中θ^（g+1)要保证大于θ（g）：
  - 即保证log p(x|θ^(g+1)) ≥ log p(x|θ^(g))
  - 因为有 log p(x|θ) = log p(x,z|θ) - log p(z|x,θ)
    - 其中的z作为生成模型的隐变量代表每个样本属于哪个高斯模型为离散取值
    - 对两边求期望，在p(z|x,θ^(g))分布下
    - 左边由于没有z不变
    - 右边得到两项记为Q(θ,θ^(g))和H(θ,θ^(g))，两式相减
      - 发现对于H()来说，应用凸函数性质可得H(θ^(g),θ^(g))≥H(θ,θ^(g))所以也≥H(θ^(g+1),θ^(g))
    - 所以只需将Q项递增即可
  - 所以可得:
    - θ^(g+1)=argmax∫z log p(x,z|θ)·p(z|x,θ^(g))

### 高斯混合模型中

通过EM使得每一步的优化只需考虑一个高斯

- **E步骤**

  - $$
    Σ_{z_1=1}^{k}{Σ_{z_2=1}^{k}{……Σ_{z_1=N}^{k}}}(Σ_{i=1}^{N}[log{α_{z_i}}+log{N(x_i|{μ_{z_i}},Σ_{z_i})}]∏_{i=1}^{N}P(z_i|x_i,θ^{(g)}))
    $$

    - 取中间项为 :
      $$
      f_i(z_i)
      $$

    - 后面项为 :                                                                                                 P(z1……zN)

    - $$
      \begin{aligned} 
      上式
      &=Σ_{z_1=1}^{k}{Σ_{z_2=1}^{k}{……Σ_{z_1=N}^{k}}}(Σ_{i=1}^{N}f_i(z_i))*P(z_1…z_N) \\
      &=Σ_{z_1=1}^{k}{Σ_{z_2=1}^{k}{……Σ_{z_1=N}^{k}}}(f_1(z_1)*p(z_1…z_N)+……\\
      &=Σ_{z_1=1}^{k}f_1(z_1)*p(z_1)+……\\
      &=Σ_{i=1}^{N}{Σ_{z_i=1}^{k}}(logα_{z_i}+logN(x_i|μ_{z_i},Σ_{z_i}))p(z_i|x_i,θ^{(g)})
      \end{aligned}
      $$

    

- **M步骤**

  - 优化α_zi参数的时候不需要N参数

    - 所以分成两部分求偏导等于0
    - 且α_zi的和为1
    - 用拉格朗日乘数法

  - 得到αl=
    $$
    (1/N)Σ_{i=1}^{n}p({l}|x_i,θ^{(g)})
    $$

  - 均值方差的优化